name: code-13b
context_size: 4096
f16: false # true to GPU acceleration
cuda: false # true to GPU acceleration
gpu_layers: 0 # this model have max 40 layers, 15-20 is reccomended for half-load at NVIDIA 4060 TiTan (more layers -- more VRAM required), (i guess 0 is no GPU)
parameters:
  model: code-13b.Q5_K_M.gguf
stopwords: 
- "</s>"
template:

  chat: &template |
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    Instruction: {{.Input}}
    Response:
  # Modify the prompt template here ^^^ as per your requirements
  completion: *template