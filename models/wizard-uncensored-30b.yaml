name: wizard-uncensored-30b
context_size: 2048
<<<<<<< HEAD
f16: true # true to GPU acceleration
=======
f16: false # true to GPU acceleration
>>>>>>> deploy/gpu-worker-p2p
cuda: true # true to GPU acceleration
gpu_layers: 10 # this model have max 40 layers, 15-20 is reccomended for half-load at NVIDIA 4060 TiTan (more layers -- more VRAM required), (i guess 0 is no GPU)
parameters:
  model: Wizard-Vicuna-30B-Uncensored.Q4_K_M.gguf
<<<<<<< HEAD
  #model: huggingface://TheBloke/Wizard-Vicuna-30B-Uncensored-GGUF/Wizard-Vicuna-30B-Uncensored.Q4_K_M.gguf
stopwords:
- "HUMAN:"
=======
>>>>>>> deploy/gpu-worker-p2p
cutstrings:
- "</s>"
template:

  chat: &template |
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    Instruction: {{.Input}}
    Response:
  # Modify the prompt template here ^^^ as per your requirements
  completion: *template
<<<<<<< HEAD
=======

  download_files: [https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GGUF/raw/main/Wizard-Vicuna-30B-Uncensored.Q4_K_M.gguf]
>>>>>>> deploy/gpu-worker-p2p
