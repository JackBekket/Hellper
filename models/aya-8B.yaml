name: aya-8B
context_size: 8192
f16: false # true to GPU acceleration
cuda: true # true to GPU acceleration
gpu_layers: 10 # this model have max 40 layers, 15-20 is reccomended for half-load at NVIDIA 4060 TiTan (more layers -- more VRAM required), (i guess 0 is no GPU)
parameters:
  model: aya-23-8B-Q5_K_S.gguf
stopwords:
- "<|END_OF_TURN_TOKEN|>"
template:

  chat: &template |
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    Instruction: {{.Input}}
    Response:
  # Modify the prompt template here ^^^ as per your requirements
  completion: *template